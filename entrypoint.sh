#!/bin/bash

# Pull the model at runtime
ollama pull llama3

# Then start the server
ollama serve
